{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T23:41:23.162013Z",
     "iopub.status.busy": "2025-04-11T23:41:23.161681Z",
     "iopub.status.idle": "2025-04-12T02:17:34.318234Z",
     "shell.execute_reply": "2025-04-12T02:17:34.317293Z",
     "shell.execute_reply.started": "2025-04-11T23:41:23.161990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter, OrderedDict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------- Logging Setup ----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------- NLP Processor ----------------------------\n",
    "class NLPProcessor:\n",
    "    \"\"\"Modern NLP processing class with efficient model loading and caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        \"\"\"Initialize the NLP processor with optional caching directory.\"\"\"\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else None\n",
    "        self.models = {}\n",
    "        \n",
    "        # Load base model (en_core_web_lg) \n",
    "        self.nlp = self._load_spacy_model(\"en_core_web_lg\")\n",
    "        \n",
    "        # Initialize the SpaCy sentencizer using English class\n",
    "        self.sentencizer = English()\n",
    "        self.sentencizer.add_pipe(\"sentencizer\")\n",
    "        \n",
    "        logger.info(\"NLP processor initialized successfully\")\n",
    "    \n",
    "    def _load_spacy_model(self, model_name: str, disable: List[str] = None) -> spacy.language.Language:\n",
    "        \"\"\"Load a SpaCy model with caching support.\"\"\"\n",
    "        if disable is None:\n",
    "            disable = []\n",
    "            \n",
    "        model_key = f\"{model_name}_{'-'.join(disable)}\"\n",
    "        \n",
    "        if model_key in self.models:\n",
    "            return self.models[model_key]\n",
    "        \n",
    "        logger.info(f\"Loading SpaCy model: {model_name} (disabled: {disable})\")\n",
    "        model = spacy.load(model_name, disable=disable)\n",
    "        self.models[model_key] = model\n",
    "        return model\n",
    "    \n",
    "    def get_model(self, task: str) -> spacy.language.Language:\n",
    "        \"\"\"Get a specialized model for a specific NLP task.\"\"\"\n",
    "        if task == \"ner\":\n",
    "            return self._load_spacy_model(\"en_core_web_lg\", disable=['parser', 'tagger'])\n",
    "        elif task == \"pos\":\n",
    "            return self._load_spacy_model(\"en_core_web_lg\", disable=['ner', 'parser'])\n",
    "        elif task in (\"parse\", \"dep\"):\n",
    "            return self._load_spacy_model(\"en_core_web_lg\", disable=['ner'])\n",
    "        elif task == \"base\":\n",
    "            return self.nlp\n",
    "        else:\n",
    "            return self.nlp\n",
    "    \n",
    "    def separate_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences using the SpaCy sentencizer.\"\"\"\n",
    "        doc = self.sentencizer(text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "        return sentences\n",
    "    \n",
    "    def lemmatize(self, word: str, is_verb: bool = False) -> str:\n",
    "        \"\"\"Lemmatize a word using SpaCy's model.\"\"\"\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            doc = self.nlp(word)\n",
    "            if len(doc) > 0:\n",
    "                return doc[0].lemma_\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SpaCy lemmatization failed: {e}\")\n",
    "        return word\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_sim(x: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        from numpy.linalg import norm\n",
    "        from numpy import dot\n",
    "        a = np.array(x)\n",
    "        b = np.array(y)\n",
    "        if norm(a) == 0 or norm(b) == 0:\n",
    "            return 0\n",
    "        return abs(dot(a, b) / (norm(a) * norm(b)))\n",
    "\n",
    "# ---------------------------- SpaCy-based SRL Processor ----------------------------\n",
    "class SpacySRLProcessor:\n",
    "    \"\"\"Semantic Role Labeling (SRL) using SpaCy dependency parsing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the SpaCy-based SRL processor.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading SpaCy model for SRL processing\")\n",
    "            self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "            if \"parser\" not in self.nlp.pipe_names:\n",
    "                logger.warning(\"Dependency parser not found in pipeline, enabling it\")\n",
    "                self.nlp.enable_pipe(\"parser\")\n",
    "            logger.info(\"SpaCy SRL processor initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize SpaCy SRL processor: {e}\")\n",
    "            self.nlp = None\n",
    "            \n",
    "    def extract_srl(self, text: str) -> Dict:\n",
    "        \"\"\"Extract semantic roles from text using SpaCy's dependency parsing.\"\"\"\n",
    "        if not self.nlp:\n",
    "            logger.error(\"SRL processor not properly initialized\")\n",
    "            return {\"words\": [], \"verbs\": []}\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        words = [token.text for token in doc]\n",
    "        verbs = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Check if token is a verb (or certain AUX tokens)\n",
    "            if token.pos_ == \"VERB\" or (token.pos_ == \"AUX\" and any(child.dep_ == \"xcomp\" for child in token.children)):\n",
    "                verb_dict = {\"verb\": token.text, \"description\": text}\n",
    "                tags = [\"O\"] * len(words)\n",
    "                \n",
    "                # Mark the verb token\n",
    "                verb_index = token.i\n",
    "                tags[verb_index] = \"B-V\"\n",
    "                \n",
    "                # Subjects (ARG0)\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in (\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\"):\n",
    "                        span = list(child.subtree)\n",
    "                        for i, span_token in enumerate(span):\n",
    "                            idx = span_token.i\n",
    "                            tags[idx] = \"B-ARG0\" if i == 0 else \"I-ARG0\"\n",
    "                \n",
    "                # Direct objects (ARG1)\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in (\"dobj\", \"attr\", \"oprd\"):\n",
    "                        span = list(child.subtree)\n",
    "                        for i, span_token in enumerate(span):\n",
    "                            idx = span_token.i\n",
    "                            tags[idx] = \"B-ARG1\" if i == 0 else \"I-ARG1\"\n",
    "                \n",
    "                # Indirect objects (ARG2)\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"iobj\":\n",
    "                        span = list(child.subtree)\n",
    "                        for i, span_token in enumerate(span):\n",
    "                            idx = span_token.i\n",
    "                            tags[idx] = \"B-ARG2\" if i == 0 else \"I-ARG2\"\n",
    "                \n",
    "                # Prepositional objects (ARG2)\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"prep\":\n",
    "                        for pobj in child.children:\n",
    "                            if pobj.dep_ == \"pobj\":\n",
    "                                span = list(pobj.subtree)\n",
    "                                prep_span = [child] + span\n",
    "                                for i, span_token in enumerate(prep_span):\n",
    "                                    idx = span_token.i\n",
    "                                    tags[idx] = \"B-ARG2\" if i == 0 else \"I-ARG2\"\n",
    "                \n",
    "                # Adverbial modifiers (e.g. temporal)\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in (\"advmod\", \"npadvmod\"):\n",
    "                        idx = child.i\n",
    "                        tags[idx] = \"B-ARGM-MNR\"\n",
    "                    if child.dep_ == \"npadvmod\" and child.head.pos_ == \"VERB\":\n",
    "                        for grandchild in child.children:\n",
    "                            if grandchild.ent_type_ in (\"DATE\", \"TIME\"):\n",
    "                                span = list(grandchild.subtree)\n",
    "                                for i, span_token in enumerate(span):\n",
    "                                    idx = span_token.i\n",
    "                                    tags[idx] = \"B-ARGM-TMP\" if i == 0 else \"I-ARGM-TMP\"\n",
    "                \n",
    "                # Create unique verb ID\n",
    "                verb_dict[\"id\"] = f\"{token.text}_{verb_index}\"\n",
    "                verb_dict[\"tags\"] = tags\n",
    "                verbs.append(verb_dict)\n",
    "        \n",
    "        return {\"words\": words, \"verbs\": verbs}\n",
    "    \n",
    "    def srl_to_dict(self, srl_output: Dict) -> Dict:\n",
    "        \"\"\"Convert SRL output into a structured dictionary format.\"\"\"\n",
    "        srl_dict = {}\n",
    "        for verb in srl_output.get('verbs', []):\n",
    "            verb_str = verb.get('id', verb.get('verb', 'unknown'))\n",
    "            srl_dict[verb_str] = {}\n",
    "            for ind, tag in enumerate(verb.get('tags', [])):\n",
    "                if tag != 'O':\n",
    "                    new_tag = tag[tag.find('-') + 1:]\n",
    "                    if tag.startswith('B'):\n",
    "                        # Start of a new role\n",
    "                        if new_tag not in srl_dict[verb_str]:\n",
    "                            srl_dict[verb_str][new_tag] = {'text': srl_output['words'][ind]}\n",
    "                        else:\n",
    "                            srl_dict[verb_str][new_tag]['text'] += f\"/ {srl_output['words'][ind]}\"\n",
    "                    else:\n",
    "                        # Continuing a role\n",
    "                        if new_tag in srl_dict[verb_str]:\n",
    "                            srl_dict[verb_str][new_tag]['text'] += f\" {srl_output['words'][ind]}\"\n",
    "        return srl_dict\n",
    "\n",
    "# ---------------------------- File Utilities ----------------------------\n",
    "class FileUtils:\n",
    "    \"\"\"Utilities for file operations such as saving and reading data.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_dict_as_json(output_path: str, data_dict: Dict, note: str = '') -> str:\n",
    "        \"\"\"Save a dictionary as JSON; optionally add a note in a separate text file.\"\"\"\n",
    "        output_path = Path(output_path)\n",
    "        with open(output_path, 'w', encoding='utf-8') as fp:\n",
    "            \n",
    "            json.dump(data_dict, fp, indent=2)\n",
    "        if note:\n",
    "            note_path = output_path.with_suffix('.txt')\n",
    "            with open(note_path, 'w', encoding='utf-8') as fp:\n",
    "                fp.write(note)\n",
    "        return 'done'\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_json_as_dict(input_path: str) -> Dict:\n",
    "        \"\"\"Read a JSON file into a dictionary.\"\"\"\n",
    "        with open(Path(input_path), 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "\n",
    "# ---------------------------- Siamese Data Preparation ----------------------------\n",
    "class SiameseDataPreparer:\n",
    "    \"\"\"Prepares data for a Siamese network for CVE-Technique matching.\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_processor: NLPProcessor, srl_processor: SpacySRLProcessor):\n",
    "        \"\"\"Initialize using NLP processor for text and SRL processor for semantic roles.\"\"\"\n",
    "        self.nlp = nlp_processor\n",
    "        self.srl = srl_processor\n",
    "    \n",
    "    def prepare_data(\n",
    "        self,\n",
    "        cve_file_path: str,\n",
    "        technique_file_path: str,\n",
    "        neg_samples_per_cve: int = 3,\n",
    "        output_path: Optional[str] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare data for Siamese network training.\n",
    "        \n",
    "        Args:\n",
    "            cve_file_path: Path to Excel file with CVE descriptions and MITRE technique numbers.\n",
    "            technique_file_path: Path to Excel file with technique IDs and their descriptions.\n",
    "            neg_samples_per_cve: Number of negative samples to generate per CVE.\n",
    "            output_path: Optional path to save the processed JSON output.\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with prepared sample pairs.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading CVE data from {cve_file_path}\")\n",
    "        df_cve = pd.read_excel(cve_file_path)\n",
    "        \n",
    "        logger.info(f\"Loading technique data from {technique_file_path}\")\n",
    "        df_tech = pd.read_excel(technique_file_path)\n",
    "        \n",
    "        # Normalize column names for the techniques file\n",
    "        df_tech.columns = [\"Technique_ID\", \"Technique_Description\"]\n",
    "        tech_dict = pd.Series(df_tech.Technique_Description.values, index=df_tech.Technique_ID).to_dict()\n",
    "        \n",
    "        samples = []\n",
    "        logger.info(\"Building Siamese pairs...\")\n",
    "        \n",
    "        for idx, row in tqdm(df_cve.iterrows(), total=len(df_cve), desc=\"Processing CVEs\"):\n",
    "            cve_text = row[\"CVE_Description\"]\n",
    "            cve_id = row.get(\"CVE_ID\", f\"Unknown_{idx}\")\n",
    "            \n",
    "            # ---- Robust handling for MITRE_Technique_Numbers ----\n",
    "            technique_list = row[\"MITRE_Technique_Numbers\"]\n",
    "            if isinstance(technique_list, str):\n",
    "                technique_list = technique_list.strip()\n",
    "                # If the string looks like a list, attempt to parse it\n",
    "                if technique_list.startswith(\"[\") and technique_list.endswith(\"]\"):\n",
    "                    try:\n",
    "                        technique_list = ast.literal_eval(technique_list)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error parsing list for {cve_id}: {technique_list}, error: {e}\")\n",
    "                        technique_list = [technique_list]\n",
    "                else:\n",
    "                    # Assume it is a single technique and wrap it in a list\n",
    "                    technique_list = [technique_list]\n",
    "            elif not isinstance(technique_list, list):\n",
    "                logger.warning(f\"Unexpected type for MITRE_Technique_Numbers for {cve_id}: {type(technique_list)}\")\n",
    "                technique_list = []\n",
    "            \n",
    "            # Create positive samples: for each technique that matches\n",
    "            for tech_id in technique_list:\n",
    "                if tech_id in tech_dict:\n",
    "                    samples.append({\n",
    "                        \"CVE_ID\": cve_id,\n",
    "                        \"CVE_text\": cve_text,\n",
    "                        \"Technique_ID\": tech_id,\n",
    "                        \"Technique_text\": tech_dict[tech_id],\n",
    "                        \"label\": 1  # Positive pair\n",
    "                    })\n",
    "            \n",
    "            # Create negative samples: choose techniques not listed for this CVE\n",
    "            negative_techs = [t for t in tech_dict.keys() if t not in technique_list]\n",
    "            if negative_techs:\n",
    "                num_neg_samples = min(neg_samples_per_cve, len(negative_techs))\n",
    "                sampled_negatives = np.random.choice(negative_techs, num_neg_samples, replace=False)\n",
    "                for tech_id in sampled_negatives:\n",
    "                    samples.append({\n",
    "                        \"CVE_ID\": cve_id,\n",
    "                        \"CVE_text\": cve_text,\n",
    "                        \"Technique_ID\": tech_id,\n",
    "                        \"Technique_text\": tech_dict[tech_id],\n",
    "                        \"label\": 0  # Negative pair\n",
    "                    })\n",
    "        \n",
    "        logger.info(f\"Generated {len(samples)} Siamese pairs.\")\n",
    "        \n",
    "        # Process sentences and extract SRL features\n",
    "        logger.info(\"Processing SRL for text pairs...\")\n",
    "        for sample in tqdm(samples, desc=\"Extracting SRL\"):\n",
    "            sample[\"CVE_sentences\"] = self.nlp.separate_sentences(sample[\"CVE_text\"])\n",
    "            sample[\"Technique_sentences\"] = self.nlp.separate_sentences(sample[\"Technique_text\"])\n",
    "            \n",
    "            sample[\"CVE_srl\"] = []\n",
    "            for sentence in sample[\"CVE_sentences\"]:\n",
    "                srl_output = self.srl.extract_srl(sentence)\n",
    "                srl_dict = self.srl.srl_to_dict(srl_output)\n",
    "                sample[\"CVE_srl\"].append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"srl_raw\": srl_output,\n",
    "                    \"srl_structured\": srl_dict\n",
    "                })\n",
    "            \n",
    "            sample[\"Technique_srl\"] = []\n",
    "            for sentence in sample[\"Technique_sentences\"]:\n",
    "                srl_output = self.srl.extract_srl(sentence)\n",
    "                srl_dict = self.srl.srl_to_dict(srl_output)\n",
    "                sample[\"Technique_srl\"].append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"srl_raw\": srl_output,\n",
    "                    \"srl_structured\": srl_dict\n",
    "                })\n",
    "            \n",
    "            # Calculate a basic verb match count between the two texts\n",
    "            sample[\"verb_match_count\"] = self._calculate_verb_match(\n",
    "                sample[\"CVE_srl\"], sample[\"Technique_srl\"]\n",
    "            )\n",
    "            \n",
    "            # Calculate role match score based on matching role texts\n",
    "            sample[\"role_match_score\"] = self._calculate_role_match_score(\n",
    "                sample[\"CVE_srl\"], sample[\"Technique_srl\"]\n",
    "            )\n",
    "        \n",
    "        # Optionally save the prepared data to a JSON file\n",
    "        if output_path:\n",
    "            output_path = Path(output_path)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            FileUtils.save_dict_as_json(\n",
    "                str(output_path),\n",
    "                {\"samples\": samples},\n",
    "                note=f\"Generated {len(samples)} Siamese pairs with {sum(s['label'] for s in samples)} positive and {len(samples) - sum(s['label'] for s in samples)} negative samples.\"\n",
    "            )\n",
    "            logger.info(f\"Data saved to: {output_path}\")\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _calculate_verb_match(self, cve_srl_list, tech_srl_list) -> int:\n",
    "        \"\"\"Calculate the number of matching verbs between CVE and Technique texts.\"\"\"\n",
    "        cve_verbs = set()\n",
    "        for srl_item in cve_srl_list:\n",
    "            for verb_id in srl_item[\"srl_structured\"].keys():\n",
    "                verb = verb_id.split('_')[0] if '_' in verb_id else verb_id\n",
    "                cve_verbs.add(self.nlp.lemmatize(verb, is_verb=True))\n",
    "        \n",
    "        tech_verbs = set()\n",
    "        for srl_item in tech_srl_list:\n",
    "            for verb_id in srl_item[\"srl_structured\"].keys():\n",
    "                verb = verb_id.split('_')[0] if '_' in verb_id else verb_id\n",
    "                tech_verbs.add(self.nlp.lemmatize(verb, is_verb=True))\n",
    "        \n",
    "        return len(cve_verbs.intersection(tech_verbs))\n",
    "    \n",
    "    def _calculate_role_match_score(self, cve_srl_list, tech_srl_list) -> float:\n",
    "        \"\"\"Calculate a similarity score based on matching semantic role texts.\"\"\"\n",
    "        cve_roles = {}\n",
    "        for srl_item in cve_srl_list:\n",
    "            for verb_id, roles in srl_item[\"srl_structured\"].items():\n",
    "                for role_type, role_info in roles.items():\n",
    "                    cve_roles.setdefault(role_type, []).append(role_info[\"text\"])\n",
    "        \n",
    "        tech_roles = {}\n",
    "        for srl_item in tech_srl_list:\n",
    "            for verb_id, roles in srl_item[\"srl_structured\"].items():\n",
    "                for role_type, role_info in roles.items():\n",
    "                    tech_roles.setdefault(role_type, []).append(role_info[\"text\"])\n",
    "        \n",
    "        role_similarities = []\n",
    "        for role_type in set(cve_roles.keys()).intersection(set(tech_roles.keys())):\n",
    "            if role_type == \"V\":  # Skip the verb role if already counted\n",
    "                continue\n",
    "            role_sim = self._calculate_text_list_similarity(cve_roles[role_type], tech_roles[role_type])\n",
    "            role_similarities.append(role_sim)\n",
    "        \n",
    "        if role_similarities:\n",
    "            return sum(role_similarities) / len(role_similarities)\n",
    "        return 0.0\n",
    "    \n",
    "    def _calculate_text_list_similarity(self, text_list1: List[str], text_list2: List[str]) -> float:\n",
    "        \"\"\"Calculate a similarity score between two lists of texts using SpaCy vectors.\"\"\"\n",
    "        max_sim = 0.0\n",
    "        for text1 in text_list1:\n",
    "            for text2 in text_list2:\n",
    "                try:\n",
    "                    doc1 = self.nlp.get_model(\"base\")(text1)\n",
    "                    doc2 = self.nlp.get_model(\"base\")(text2)\n",
    "                    if doc1.vector_norm and doc2.vector_norm:\n",
    "                        sim = doc1.similarity(doc2)\n",
    "                        max_sim = max(max_sim, sim)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error calculating similarity: {e}\")\n",
    "        return max_sim\n",
    "\n",
    "# ---------------------------- Main Execution ----------------------------\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Initialize processors\n",
    "    nlp_processor = NLPProcessor()\n",
    "    srl_processor = SpacySRLProcessor()\n",
    "    data_preparer = SiameseDataPreparer(nlp_processor, srl_processor)\n",
    "    \n",
    "    # Define input and output file paths (adjust these paths as needed)\n",
    "    cve_file_path = \"cve_single_technique.xlsx\"\n",
    "    technique_file_path = \"techniques.xlsx\"\n",
    "    output_path = \"siamese_samples_with_srl.json\"\n",
    "    \n",
    "    # Create output directory if necessary\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Prepare data\n",
    "    samples = data_preparer.prepare_data(\n",
    "        cve_file_path=cve_file_path,\n",
    "        technique_file_path=technique_file_path,\n",
    "        neg_samples_per_cve=3,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    # Print summary statistics\n",
    "    positive_count = sum(1 for s in samples if s[\"label\"] == 1)\n",
    "    negative_count = len(samples) - positive_count\n",
    "    avg_verb_match_positive = (sum(s[\"verb_match_count\"] for s in samples if s[\"label\"] == 1) / positive_count) if positive_count else 0\n",
    "    avg_verb_match_negative = (sum(s[\"verb_match_count\"] for s in samples if s[\"label\"] == 0) / negative_count) if negative_count else 0\n",
    "    avg_role_match_positive = (sum(s[\"role_match_score\"] for s in samples if s[\"label\"] == 1) / positive_count) if positive_count else 0\n",
    "    avg_role_match_negative = (sum(s[\"role_match_score\"] for s in samples if s[\"label\"] == 0) / negative_count) if negative_count else 0\n",
    "    \n",
    "    logger.info(\"Data preparation complete:\")\n",
    "    logger.info(f\"  - Total samples: {len(samples)}\")\n",
    "    logger.info(f\"  - Positive pairs: {positive_count}\")\n",
    "    logger.info(f\"  - Negative pairs: {negative_count}\")\n",
    "    logger.info(f\"  - Average verb matches (positive pairs): {avg_verb_match_positive:.2f}\")\n",
    "    logger.info(f\"  - Average verb matches (negative pairs): {avg_verb_match_negative:.2f}\")\n",
    "    logger.info(f\"  - Average role match score (positive pairs): {avg_role_match_positive:.2f}\")\n",
    "    logger.info(f\"  - Average role match score (negative pairs): {avg_role_match_negative:.2f}\")\n",
    "    logger.info(f\"  - Data saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T13:06:27.740982Z",
     "iopub.status.busy": "2025-04-11T13:06:27.740587Z",
     "iopub.status.idle": "2025-04-11T13:06:27.750671Z",
     "shell.execute_reply": "2025-04-11T13:06:27.749681Z",
     "shell.execute_reply.started": "2025-04-11T13:06:27.740947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T00:33:45.410782Z",
     "iopub.status.busy": "2025-04-05T00:33:45.410472Z",
     "iopub.status.idle": "2025-04-05T00:33:45.418629Z",
     "shell.execute_reply": "2025-04-05T00:33:45.417716Z",
     "shell.execute_reply.started": "2025-04-05T00:33:45.410759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T21:34:01.742557Z",
     "iopub.status.busy": "2025-04-11T21:34:01.742254Z",
     "iopub.status.idle": "2025-04-11T21:34:31.117186Z",
     "shell.execute_reply": "2025-04-11T21:34:31.116307Z",
     "shell.execute_reply.started": "2025-04-11T21:34:01.742535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6898833,
     "sourceId": 11070665,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6914149,
     "sourceId": 11091775,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6981612,
     "sourceId": 11184501,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7109113,
     "sourceId": 11358944,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
